{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690, 30, 79)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "\"ㅇ_2\", \n",
    "\"ㅎ\"\n",
    "]\n",
    "\n",
    "data = np.concatenate([\n",
    "    np.load('dataset/seq_ㅇ_2_1636341560.npy'),\n",
    "    np.load('dataset/seq_ㅎ_1636341560.npy')\n",
    "    \n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1690, 30, 78)\n",
      "(1690,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521, 30, 78) (1521, 2)\n",
      "(169, 30, 78) (169, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 64)                36608     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 38,754\n",
      "Trainable params: 38,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3172 - acc: 0.8619 - val_loss: 0.0116 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to models\\model_j24.h5\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 9.3334e-04 - acc: 1.0000 - val_loss: 1.1392e-06 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 4.5747e-07 - acc: 1.0000 - val_loss: 8.8523e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 3.8200e-07 - acc: 1.0000 - val_loss: 7.9142e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 1.00000\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.3529e-07 - acc: 1.0000 - val_loss: 6.9972e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 1.00000\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.9257e-07 - acc: 1.0000 - val_loss: 6.1508e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 1.00000\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5597e-07 - acc: 1.0000 - val_loss: 5.4454e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 1.00000\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2478e-07 - acc: 1.0000 - val_loss: 4.8741e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 1.00000\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9876e-07 - acc: 1.0000 - val_loss: 4.3733e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 1.00000\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.7650e-07 - acc: 1.0000 - val_loss: 3.9148e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 1.00000\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5738e-07 - acc: 1.0000 - val_loss: 3.5128e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 1.00000\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4068e-07 - acc: 1.0000 - val_loss: 3.2094e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 1.00000\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2665e-07 - acc: 1.0000 - val_loss: 2.8991e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 1.00000\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.1435e-07 - acc: 1.0000 - val_loss: 2.6381e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 1.00000\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0400e-07 - acc: 1.0000 - val_loss: 2.4194e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 1.00000\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 9.4521e-08 - acc: 1.0000 - val_loss: 2.2290e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 1.00000\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.6056e-08 - acc: 1.0000 - val_loss: 2.0526e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 1.00000\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8767e-08 - acc: 1.0000 - val_loss: 1.8834e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 1.00000\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.2027e-08 - acc: 1.0000 - val_loss: 1.7564e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 1.00000\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.6462e-08 - acc: 1.0000 - val_loss: 1.6365e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 1.00000\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.1525e-08 - acc: 1.0000 - val_loss: 1.5166e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 1.00000\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.6509e-08 - acc: 1.0000 - val_loss: 1.4108e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 1.00000\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.2747e-08 - acc: 1.0000 - val_loss: 1.3120e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 1.00000\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.9220e-08 - acc: 1.0000 - val_loss: 1.2274e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 1.00000\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.5771e-08 - acc: 1.0000 - val_loss: 1.1498e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 1.00000\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.2558e-08 - acc: 1.0000 - val_loss: 1.0792e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 1.00000\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.9971e-08 - acc: 1.0000 - val_loss: 1.0157e-07 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 1.00000\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.7463e-08 - acc: 1.0000 - val_loss: 9.5931e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 1.00000\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.5034e-08 - acc: 1.0000 - val_loss: 8.9583e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 1.00000\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2761e-08 - acc: 1.0000 - val_loss: 8.5351e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 1.00000\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.1272e-08 - acc: 1.0000 - val_loss: 8.0413e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 1.00000\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8921e-08 - acc: 1.0000 - val_loss: 7.6181e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 1.00000\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.7745e-08 - acc: 1.0000 - val_loss: 7.2654e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 1.00000\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5786e-08 - acc: 1.0000 - val_loss: 6.9127e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 1.00000\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.4453e-08 - acc: 1.0000 - val_loss: 6.5600e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 1.00000\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.3278e-08 - acc: 1.0000 - val_loss: 6.3484e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 1.00000\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.1867e-08 - acc: 1.0000 - val_loss: 5.9957e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 1.00000\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.0613e-08 - acc: 1.0000 - val_loss: 5.7136e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 1.00000\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9986e-08 - acc: 1.0000 - val_loss: 5.3609e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 1.00000\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9124e-08 - acc: 1.0000 - val_loss: 5.1493e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 1.00000\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.7870e-08 - acc: 1.0000 - val_loss: 4.9377e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 1.00000\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.7086e-08 - acc: 1.0000 - val_loss: 4.7260e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 1.00000\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.6302e-08 - acc: 1.0000 - val_loss: 4.5144e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 1.00000\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5440e-08 - acc: 1.0000 - val_loss: 4.3028e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 1.00000\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4813e-08 - acc: 1.0000 - val_loss: 4.0912e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 1.00000\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4029e-08 - acc: 1.0000 - val_loss: 3.9501e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 1.00000\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3637e-08 - acc: 1.0000 - val_loss: 3.8090e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 1.00000\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2618e-08 - acc: 1.0000 - val_loss: 3.5974e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2305e-08 - acc: 1.0000 - val_loss: 3.4564e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.1678e-08 - acc: 1.0000 - val_loss: 3.3858e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.1129e-08 - acc: 1.0000 - val_loss: 3.1037e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0659e-08 - acc: 1.0000 - val_loss: 3.1037e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 1.00000\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-08 - acc: 1.0000 - val_loss: 2.9626e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 1.00000\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0032e-08 - acc: 1.0000 - val_loss: 2.9626e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 1.00000\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 9.6402e-09 - acc: 1.0000 - val_loss: 2.8921e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 1.00000\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 9.4051e-09 - acc: 1.0000 - val_loss: 2.8921e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 1.00000\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 9.2483e-09 - acc: 1.0000 - val_loss: 2.8215e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 1.00000\n",
      "Epoch 58/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 9.1699e-09 - acc: 1.0000 - val_loss: 2.6804e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 1.00000\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.7781e-09 - acc: 1.0000 - val_loss: 2.6804e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 1.00000\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.6213e-09 - acc: 1.0000 - val_loss: 2.6099e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 1.00000\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.6213e-09 - acc: 1.0000 - val_loss: 2.5394e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 1.00000\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.3862e-09 - acc: 1.0000 - val_loss: 2.5394e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 1.00000\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.2294e-09 - acc: 1.0000 - val_loss: 2.4688e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 1.00000\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.7592e-09 - acc: 1.0000 - val_loss: 2.3983e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 1.00000\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.5241e-09 - acc: 1.0000 - val_loss: 2.3983e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 1.00000\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.4457e-09 - acc: 1.0000 - val_loss: 2.3278e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 1.00000\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 7.4457e-09 - acc: 1.0000 - val_loss: 2.2572e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 1.00000\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.1322e-09 - acc: 1.0000 - val_loss: 2.1867e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 1.00000\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.0538e-09 - acc: 1.0000 - val_loss: 2.1161e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 1.00000\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.9754e-09 - acc: 1.0000 - val_loss: 2.1161e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 1.00000\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.8971e-09 - acc: 1.0000 - val_loss: 2.1161e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 1.00000\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.7403e-09 - acc: 1.0000 - val_loss: 2.0456e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 1.00000\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.5052e-09 - acc: 1.0000 - val_loss: 1.9751e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 1.00000\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.2700e-09 - acc: 1.0000 - val_loss: 1.9751e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 1.00000\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.9565e-09 - acc: 1.0000 - val_loss: 1.9751e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 1.00000\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.9565e-09 - acc: 1.0000 - val_loss: 1.9045e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 1.00000\n",
      "Epoch 77/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.8782e-09 - acc: 1.0000 - val_loss: 1.8340e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 1.00000\n",
      "Epoch 78/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.7214e-09 - acc: 1.0000 - val_loss: 1.7635e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 1.00000\n",
      "Epoch 79/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.4863e-09 - acc: 1.0000 - val_loss: 1.7635e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 1.00000\n",
      "Epoch 80/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.4863e-09 - acc: 1.0000 - val_loss: 1.6929e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 1.00000\n",
      "Epoch 81/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.3295e-09 - acc: 1.0000 - val_loss: 1.6929e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 1.00000\n",
      "Epoch 82/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 5.2512e-09 - acc: 1.0000 - val_loss: 1.6224e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 1.00000\n",
      "Epoch 83/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.9377e-09 - acc: 1.0000 - val_loss: 1.6224e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 1.00000\n",
      "Epoch 84/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.8593e-09 - acc: 1.0000 - val_loss: 1.5518e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 1.00000\n",
      "Epoch 85/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.7809e-09 - acc: 1.0000 - val_loss: 1.5518e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 1.00000\n",
      "Epoch 86/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.7809e-09 - acc: 1.0000 - val_loss: 1.5518e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 1.00000\n",
      "Epoch 87/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.7809e-09 - acc: 1.0000 - val_loss: 1.5518e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 1.00000\n",
      "Epoch 88/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.4674e-09 - acc: 1.0000 - val_loss: 1.4108e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 1.00000\n",
      "Epoch 89/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.3107e-09 - acc: 1.0000 - val_loss: 1.4108e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 1.00000\n",
      "Epoch 90/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.2323e-09 - acc: 1.0000 - val_loss: 1.4108e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 1.00000\n",
      "Epoch 91/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 4.0755e-09 - acc: 1.0000 - val_loss: 1.4108e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 1.00000\n",
      "Epoch 92/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 3.9972e-09 - acc: 1.0000 - val_loss: 1.2697e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 1.00000\n",
      "Epoch 93/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.7620e-09 - acc: 1.0000 - val_loss: 1.2697e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 1.00000\n",
      "Epoch 94/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.7620e-09 - acc: 1.0000 - val_loss: 1.1991e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 1.00000\n",
      "Epoch 95/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.7620e-09 - acc: 1.0000 - val_loss: 1.1991e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 1.00000\n",
      "Epoch 96/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.7620e-09 - acc: 1.0000 - val_loss: 1.1991e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 1.00000\n",
      "Epoch 97/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.6053e-09 - acc: 1.0000 - val_loss: 1.1286e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 1.00000\n",
      "Epoch 98/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.5269e-09 - acc: 1.0000 - val_loss: 1.1286e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 1.00000\n",
      "Epoch 99/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2918e-09 - acc: 1.0000 - val_loss: 1.1286e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 1.00000\n",
      "Epoch 100/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2918e-09 - acc: 1.0000 - val_loss: 1.1286e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 1.00000\n",
      "Epoch 101/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2918e-09 - acc: 1.0000 - val_loss: 1.0581e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 102/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2134e-09 - acc: 1.0000 - val_loss: 1.0581e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 1.00000\n",
      "Epoch 103/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.2134e-09 - acc: 1.0000 - val_loss: 1.0581e-08 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 1.00000\n",
      "Epoch 104/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.1350e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 1.00000\n",
      "Epoch 105/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.1350e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 1.00000\n",
      "Epoch 106/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.1350e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 1.00000\n",
      "Epoch 107/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.0566e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 1.00000\n",
      "Epoch 108/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 3.0566e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 1.00000\n",
      "Epoch 109/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.9783e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 1.00000\n",
      "Epoch 110/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8999e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 1.00000\n",
      "Epoch 111/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8999e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 1.00000\n",
      "Epoch 112/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8215e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 1.00000\n",
      "Epoch 113/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8215e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 1.00000\n",
      "Epoch 114/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.8215e-09 - acc: 1.0000 - val_loss: 9.8753e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 1.00000\n",
      "Epoch 115/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 2.7431e-09 - acc: 1.0000 - val_loss: 9.1699e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 1.00000\n",
      "Epoch 116/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5080e-09 - acc: 1.0000 - val_loss: 8.4646e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 1.00000\n",
      "Epoch 117/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5080e-09 - acc: 1.0000 - val_loss: 8.4646e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 1.00000\n",
      "Epoch 118/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5080e-09 - acc: 1.0000 - val_loss: 8.4646e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 1.00000\n",
      "Epoch 119/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.5080e-09 - acc: 1.0000 - val_loss: 8.4646e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 1.00000\n",
      "Epoch 120/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.4296e-09 - acc: 1.0000 - val_loss: 8.4646e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 1.00000\n",
      "Epoch 121/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.4296e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 1.00000\n",
      "Epoch 122/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.3513e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 1.00000\n",
      "Epoch 123/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2729e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 1.00000\n",
      "Epoch 124/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2729e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 1.00000\n",
      "Epoch 125/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.2729e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 1.00000\n",
      "Epoch 126/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.1161e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 1.00000\n",
      "Epoch 127/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.1161e-09 - acc: 1.0000 - val_loss: 7.7592e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 1.00000\n",
      "Epoch 128/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.1161e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 1.00000\n",
      "Epoch 129/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 2.0378e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 1.00000\n",
      "Epoch 130/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9594e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 1.00000\n",
      "Epoch 131/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9594e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 1.00000\n",
      "Epoch 132/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9594e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 1.00000\n",
      "Epoch 133/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.9594e-09 - acc: 1.0000 - val_loss: 7.0538e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 1.00000\n",
      "Epoch 134/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.8810e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 1.00000\n",
      "Epoch 135/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 1.8810e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 1.00000\n",
      "Epoch 136/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.8810e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 1.00000\n",
      "Epoch 137/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.8026e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 1.00000\n",
      "Epoch 138/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.6459e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 1.00000\n",
      "Epoch 139/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.6459e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 1.00000\n",
      "Epoch 140/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.6459e-09 - acc: 1.0000 - val_loss: 6.3484e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 1.00000\n",
      "Epoch 141/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 1.6459e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 1.00000\n",
      "Epoch 142/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.6459e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 1.00000\n",
      "Epoch 143/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 1.00000\n",
      "Epoch 144/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 1.00000\n",
      "Epoch 145/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 1.00000\n",
      "Epoch 146/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 1.00000\n",
      "Epoch 147/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 1.00000\n",
      "Epoch 148/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 1.00000\n",
      "Epoch 149/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.5675e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 1.00000\n",
      "Epoch 150/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 1.00000\n",
      "Epoch 151/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 152/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 1.00000\n",
      "Epoch 153/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 1.00000\n",
      "Epoch 154/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 1.00000\n",
      "Epoch 155/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 1.00000\n",
      "Epoch 156/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 5.6430e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 1.00000\n",
      "Epoch 157/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 4.9377e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 1.00000\n",
      "Epoch 158/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 4.9377e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 1.00000\n",
      "Epoch 159/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 4.9377e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 1.00000\n",
      "Epoch 160/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 4.9377e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 1.00000\n",
      "Epoch 161/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.4108e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 1.00000\n",
      "Epoch 162/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 1.00000\n",
      "Epoch 163/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 1.00000\n",
      "Epoch 164/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 1.00000\n",
      "Epoch 165/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 1.00000\n",
      "Epoch 166/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 1.00000\n",
      "Epoch 167/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 1.00000\n",
      "Epoch 168/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 1.00000\n",
      "Epoch 169/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 1.00000\n",
      "Epoch 170/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.3324e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 1.00000\n",
      "Epoch 171/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 1.00000\n",
      "Epoch 172/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 1.00000\n",
      "Epoch 173/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 1.00000\n",
      "Epoch 174/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 1.00000\n",
      "Epoch 175/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 1.00000\n",
      "Epoch 176/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 1.00000\n",
      "Epoch 177/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.2540e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 1.00000\n",
      "Epoch 178/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.1756e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 1.00000\n",
      "Epoch 179/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 1.0973e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 1.00000\n",
      "Epoch 180/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 1.0973e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 1.00000\n",
      "Epoch 181/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0973e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 1.00000\n",
      "Epoch 182/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0973e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 1.00000\n",
      "Epoch 183/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0973e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 1.00000\n",
      "Epoch 184/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 1.00000\n",
      "Epoch 185/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 1.00000\n",
      "Epoch 186/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 1.00000\n",
      "Epoch 187/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 1.00000\n",
      "Epoch 188/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 4.2323e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 1.00000\n",
      "Epoch 189/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 1.00000\n",
      "Epoch 190/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 1.00000\n",
      "Epoch 191/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 1.0189e-09 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 1.00000\n",
      "Epoch 192/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 8.6213e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 1.00000\n",
      "Epoch 193/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 1.00000\n",
      "Epoch 194/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 1.00000\n",
      "Epoch 195/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 1.00000\n",
      "Epoch 196/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 1.00000\n",
      "Epoch 197/200\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 3.5269e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 1.00000\n",
      "Epoch 198/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 2.1161e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 1.00000\n",
      "Epoch 199/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 7.8376e-10 - acc: 1.0000 - val_loss: 2.1161e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 1.00000\n",
      "Epoch 200/200\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 6.2700e-10 - acc: 1.0000 - val_loss: 2.1161e-09 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model_j24.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[79,  0],\n",
       "        [ 0, 90]],\n",
       "\n",
       "       [[90,  0],\n",
       "        [ 0, 79]]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model_j24.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dropl\\AppData\\Local\\Temp\\tmp_xjzpbdk\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dropl\\AppData\\Local\\Temp\\tmp_xjzpbdk\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('models/model_j24.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbc6ecb72ef9aab1ba0fcc0220f9f100166d86f33532edecf34b9fe6ae1fcfee"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
